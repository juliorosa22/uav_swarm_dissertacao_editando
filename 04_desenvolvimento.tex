% =========================================================
% Capítulo 4 – Desenvolvimento do Trabalho
% =========================================================

\chapter{Desenvolvimento do Trabalho}
\label{cap:desenvolvimento}

Este capítulo apresenta o desenvolvimento do trabalho proposto, abordando de forma detalhada a modelagem do problema de navegação cooperativa de enxames de veículos aéreos não tripulados. São descritos os ambientes de simulação utilizados, a especificação das tarefas atribuídas aos agentes, a definição dos espaços de estados e ações, a formulação das funções de recompensa, bem como os algoritmos de aprendizado por reforço multiagente e as estratégias de treinamento empregadas ao longo do estudo.

Inicialmente, o trabalho teve como objetivo utilizar exclusivamente o simulador AirSim como plataforma de desenvolvimento e experimentação. Essa escolha fundamentou-se na proposta de aprender uma política de controle de alto nível para coordenação de enxames, de modo a facilitar uma possível transferência de aprendizado para plataformas reais de VANTs. Nesse contexto, o aprendizado não teria como finalidade o controle de baixo nível, como a estabilização do voo, mas sim a coordenação estratégica dos agentes por meio do envio de comandos abstratos aos controladores de voo embarcados.

O simulador AirSim fornece uma camada de abstração compatível com controladores amplamente utilizados, como ArduPilot e PX4, permitindo que a política aprendida atue de forma semelhante ao que ocorreria em um cenário real de voo autônomo baseado em scripts de missão. Dessa forma, o ambiente de simulação possibilitou o estudo detalhado de aspectos relacionados à modelagem de tarefas cooperativas, percepção do ambiente e coordenação entre múltiplos agentes em cenários tridimensionais realistas.

Entretanto, conforme será discutido na seção de comparação entre os ambientes de simulação, tornou-se necessário adotar uma plataforma alternativa para a realização dos experimentos finais, em função da aproximação do cronograma de conclusão do trabalho e das demandas computacionais associadas ao treinamento de algoritmos de aprendizado por reforço multiagente. O simulador IsaacSim, aliado ao framework IsaacLab, apresentou desempenho computacional superior, maior escalabilidade e melhor suporte à paralelização de ambientes, possibilitando a obtenção de resultados mais consistentes e a prototipação rápida de diferentes configurações experimentais.

Dessa forma, o IsaacSim foi selecionado como a plataforma principal para o treinamento final dos modelos e a apresentação dos resultados deste trabalho. Ressalta-se, contudo, que a utilização prévia do AirSim desempenhou um papel fundamental no andamento da pesquisa, especialmente por sua especialização em aplicações envolvendo VANTs. O uso desse simulador contribuiu de maneira significativa para o entendimento do problema, para a validação conceitual das abordagens propostas e para a definição da metodologia adotada nos experimentos finais.

% =========================================================
\section{Modelagem do Problema}
\label{sec:modelagem_problema}

Esta seção apresenta a modelagem formal do problema de navegação cooperativa de enxames de VANTs, contemplando os ambientes de simulação adotados, a definição das tarefas, os espaços de estados e ações, e as funções de recompensa utilizadas no processo de aprendizado.
% TODO  - Fazer considerções gerais na modelagem formal do problema.
% Considerações: Discorrer sobre o objetivo da modelagem tentar representar um enxame de drones descentralizados. ou seja
% Cada drone deve tomar suas próprias decisões baseadas em suas observações locais e comunicação limitada com outros drones.
% Discutir sobre as limitações de sensores, gps, comunicação e ruídos nas leituras.
% ---------------------------------------------------------
\subsection{Ambientes de Simulação}
\label{subsec:ambientes_simulacao}

A utilização de ambientes de simulação fidedignos e escaláveis é um fator determinante no desenvolvimento e validação de algoritmos de aprendizado por reforço aplicados ao controle de enxames de veículos aéreos não tripulados. Neste trabalho, dois simuladores foram empregados ao longo das diferentes etapas de desenvolvimento: o AirSim e o IsaacSim. Cada um desses ambientes apresenta características específicas, tendo sido concebidos com finalidades distintas, o que impacta diretamente sua adequação a diferentes fases do processo experimental.

O AirSim é um simulador de código aberto desenvolvido inicialmente pela Microsoft Research, apresentado por Shah et al.~\cite{shah2018airsim}, com o objetivo de apoiar pesquisas em veículos autônomos, incluindo carros e VANTs. Construído sobre o motor gráfico Unreal Engine, o AirSim oferece ambientes tridimensionais de alta fidelidade visual, suporte a sensores realistas — como câmeras RGB, profundidade, sensores de distância e IMU — e integração direta com controladores de voo amplamente utilizados, como ArduPilot e PX4. Essa arquitetura permite que o simulador funcione como uma camada de abstração entre algoritmos de alto nível e controladores de baixo nível, aproximando o comportamento do sistema simulado daquele observado em operações reais.

Entre os principais pontos fortes do AirSim destacam-se sua especialização em aplicações com VANTs, a facilidade de configuração de sensores e cenários personalizados, bem como a possibilidade de realizar testes de voo autônomo baseados em scripts e missões pré-programadas. Por outro lado, o simulador apresenta limitações relacionadas ao desempenho computacional e à escalabilidade, especialmente em cenários que envolvem múltiplos agentes e demandam a execução paralela de diversos ambientes, o que pode comprometer a eficiência do treinamento de algoritmos de aprendizado por reforço multiagente.

O IsaacSim, por sua vez, é um simulador desenvolvido pela NVIDIA, fundamentado na plataforma Omniverse e apresentado como uma solução voltada à simulação robótica de alto desempenho~\cite{makoviychuk2021isaac}. Diferentemente do AirSim, o IsaacSim foi projetado desde sua concepção para oferecer escalabilidade, paralelização massiva e integração nativa com bibliotecas de aprendizado profundo. O simulador utiliza tecnologias como USD (Universal Scene Description), PhysX para simulação física e RTX para renderização acelerada por hardware, possibilitando a execução simultânea de centenas ou milhares de ambientes em GPU.

Os principais pontos fortes do IsaacSim incluem seu elevado desempenho computacional, suporte robusto à paralelização de ambientes, fidelidade física e integração direta com frameworks de aprendizado por reforço, como o IsaacLab. Essas características tornam o simulador particularmente adequado para o treinamento intensivo de políticas de controle baseadas em aprendizado por reforço, especialmente em contextos multiagentes. Em contrapartida, o IsaacSim apresenta uma curva de aprendizado mais acentuada e menor especialização nativa em aplicações aeronáuticas quando comparado ao AirSim, exigindo maior esforço na modelagem de dinâmicas específicas de VANTs.

A Figura~\ref{fig:formacao_v_airsim} ilustra um cenário de enxame de VANTs em formação em V modelado no ambiente AirSim, enquanto a Figura~\ref{fig:formacao_v_isaacsim} apresenta uma configuração equivalente implementada no IsaacSim.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/swarm_airsim.png}
    \caption{Enxame em formação em V no ambiente AirSim.}
    \label{fig:formacao_v_airsim}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/isaac-sim-env.png}
    \caption{Enxame em formação em V no ambiente IsaacSim.}
    \label{fig:formacao_v_isaacsim}
\end{figure}

No que se refere à integração com bibliotecas de aprendizado por reforço, ambos os simuladores oferecem suporte a diferentes níveis de abstração. O AirSim disponibiliza APIs em Python que permitem sua integração com frameworks como Gymnasium, Stable-Baselines e PyTorch, embora essa integração exija, em geral, a implementação manual de wrappers para adaptação ao paradigma de ambientes do tipo Markov Decision Process (MDP). Essa característica torna o AirSim mais flexível, porém menos otimizado para treinamentos em larga escala.

O IsaacSim, por outro lado, fornece integração nativa com o ecossistema de aprendizado por reforço da NVIDIA por meio do IsaacLab, que implementa interfaces compatíveis com o Gymnasium e suporta diretamente bibliotecas como PyTorch. Essa integração facilita a definição de ambientes vetorizados, o gerenciamento de observações e recompensas em larga escala, bem como a implementação de algoritmos de aprendizado por reforço multiagente de forma eficiente e modular. Em razão dessas características, o IsaacSim foi adotado como a plataforma principal para o treinamento final dos modelos e a avaliação quantitativa dos resultados apresentados neste trabalho.

% ---------------------------------------------------------
\subsection{Especificação das Tarefas dos Agentes}
\label{subsec:especificacao_tarefas}

% Descrição da missão de navegação em formação em V
% Objetivos individuais e coletivos
% Desvio de obstáculos
% Condições de sucesso e falha
% Critérios de término do episódio
% Condições de reset
% Sensores utilizados pelos agentes

% ---------------------------------------------------------
\subsection{Espaço de Estados e Ações}
\label{subsec:espaco_estados_acoes}

% Definição formal do vetor de observações
% Informações proprioceptivas
% Informações relativas aos vizinhos
% Sensores de percepção do ambiente
% Espaço de ações contínuo
% Diferenças de modelagem entre AirSim e IsaacSim

% ---------------------------------------------------------
\subsection{Funções de Recompensa}
\label{subsec:funcoes_recompensa}

% Formulação matemática das recompensas
% Termos individuais
% Termos coletivos
% Penalizações
% Reward Machines
% Estados, transições e eventos
% Integração das RM ao processo de aprendizado multiagente

% =========================================================
\section{Algoritmo de Aprendizado}
\label{sec:algoritmo_aprendizado}

Esta seção descreve o algoritmo de aprendizado por reforço multiagente utilizado, bem como sua implementação computacional nos diferentes ambientes de simulação.

% ---------------------------------------------------------
\subsection{MAPPO: Formulação e Pseudocódigo}
\label{subsec:mappo_pseudocodigo}

% Centralized Training with Decentralized Execution (CTDE)
% Formulação do MAPPO
% Função objetivo
% Atualização das políticas
% Apresentação do pseudocódigo comentado

% ---------------------------------------------------------
\subsection{Implementação Computacional}
\label{subsec:implementacao}

% Implementação no TorchRL
% Implementação no IsaacLab
% Arquiteturas neurais adotadas
% Attention + CNN no AirSim
% MLP no IsaacSim
% Justificativas das escolhas arquiteturais

% =========================================================
\section{Estratégias de Treinamento}
\label{sec:estrategias_treinamento}

Esta seção apresenta as estratégias de treinamento adotadas, incluindo a comparação entre simuladores e as abordagens baseadas em aprendizado curricular com e sem o uso de Reward Machines.

% ---------------------------------------------------------
\subsection{Treinamento no AirSim}
\label{subsec:treinamento_airsim}

% Objetivo exploratório
% Limitações computacionais
% Configuração dos experimentos
% Observações iniciais

% ---------------------------------------------------------
\subsection{Treinamento no IsaacLab}
\label{subsec:treinamento_isaaclab}

% Configuração final de treinamento
% Paralelização
% Estabilidade e escalabilidade
% Vantagens observadas

% ---------------------------------------------------------
\subsection{Comparação de Performance entre Simuladores}
\label{subsec:comparacao_simuladores}

% Comparação qualitativa
% Comparação quantitativa
% Impacto no tempo de treinamento
% Discussão sobre fidelidade física

% ---------------------------------------------------------
\subsection{Abordagem Baseline}
\label{subsec:baseline}

% Curriculum Learning sem Reward Machines
% Definição dos estágios
% Objetivo da baseline

% ---------------------------------------------------------
\subsection{Curriculum Learning com Reward Machines}
\label{subsec:curriculum_rm}

% Integração entre Curriculum Learning e RM
% Definição formal dos estágios:
% 1. Hover
% 2. Point-to-Point
% 3. Desvio de obstáculos (single-agent)
% 4. Navegação em formação em V
% 5. Navegação em enxame com obstáculos
% Discussão da progressão entre estágios
