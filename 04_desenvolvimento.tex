% =========================================================
% Capítulo 4 – Desenvolvimento do Trabalho
% =========================================================

\chapter{Desenvolvimento do Trabalho}
\label{cap:desenvolvimento}

Este capítulo apresenta o desenvolvimento do trabalho proposto, abordando de forma detalhada a modelagem do problema de navegação cooperativa de enxames de veículos aéreos não tripulados. São descritos os ambientes de simulação utilizados, a especificação das tarefas atribuídas aos agentes, a definição dos espaços de estados e ações, a formulação das funções de recompensa, bem como os algoritmos de aprendizado por reforço multiagente e as estratégias de treinamento empregadas ao longo do estudo.

Inicialmente, o trabalho teve como objetivo utilizar exclusivamente o simulador AirSim como plataforma de desenvolvimento e experimentação. Essa escolha fundamentou-se na proposta de aprender uma política de controle de alto nível para coordenação de enxames, de modo a facilitar uma possível transferência de aprendizado para plataformas reais de VANTs. Nesse contexto, o aprendizado não teria como finalidade o controle de baixo nível, como a estabilização do voo, mas sim a coordenação estratégica dos agentes por meio do envio de comandos abstratos aos controladores de voo embarcados.

O simulador AirSim fornece uma camada de abstração compatível com controladores amplamente utilizados, como ArduPilot e PX4, permitindo que a política aprendida atue de forma semelhante ao que ocorreria em um cenário real de voo autônomo baseado em scripts de missão. Dessa forma, o ambiente de simulação possibilitou o estudo detalhado de aspectos relacionados à modelagem de tarefas cooperativas, percepção do ambiente e coordenação entre múltiplos agentes em cenários tridimensionais realistas.

Entretanto, conforme será discutido na seção de comparação entre os ambientes de simulação, tornou-se necessário adotar uma plataforma alternativa para a realização dos experimentos finais, em função da aproximação do cronograma de conclusão do trabalho e das demandas computacionais associadas ao treinamento de algoritmos de aprendizado por reforço multiagente. O simulador IsaacSim, aliado ao framework IsaacLab, apresentou desempenho computacional superior, maior escalabilidade e melhor suporte à paralelização de ambientes, possibilitando a obtenção de resultados mais consistentes e a prototipação rápida de diferentes configurações experimentais.

Dessa forma, o IsaacSim foi selecionado como a plataforma principal para o treinamento final dos modelos e a apresentação dos resultados deste trabalho. Ressalta-se, contudo, que a utilização prévia do AirSim desempenhou um papel fundamental no andamento da pesquisa, especialmente por sua especialização em aplicações envolvendo VANTs. O uso desse simulador contribuiu de maneira significativa para o entendimento do problema, para a validação conceitual das abordagens propostas e para a definição da metodologia adotada nos experimentos finais.

% =========================================================
\section{Modelagem do Problema}
\label{sec:modelagem_problema}

Esta seção apresenta a modelagem formal do problema de navegação cooperativa de enxames de VANTs, contemplando os ambientes de simulação adotados, a definição das tarefas, os espaços de estados e ações, e as funções de recompensa utilizadas no processo de aprendizado.
% TODO  - Fazer considerções gerais na modelagem formal do problema.
% Considerações: Discorrer sobre o objetivo da modelagem tentar representar um enxame de drones descentralizados. ou seja
% Cada drone deve tomar suas próprias decisões baseadas em suas observações locais e comunicação limitada com outros drones.
% Discutir sobre as limitações de sensores, gps, comunicação e ruídos nas leituras.
% ---------------------------------------------------------
\subsection{Ambientes de Simulação}
\label{subsec:ambientes_simulacao}

A utilização de ambientes de simulação fidedignos e escaláveis é um fator determinante no desenvolvimento e validação de algoritmos de aprendizado por reforço aplicados ao controle de enxames de veículos aéreos não tripulados. Neste trabalho, dois simuladores foram empregados ao longo das diferentes etapas de desenvolvimento: o AirSim e o IsaacSim. Cada um desses ambientes apresenta características específicas, tendo sido concebidos com finalidades distintas, o que impacta diretamente sua adequação a diferentes fases do processo experimental.

O AirSim é um simulador de código aberto desenvolvido inicialmente pela Microsoft Research, apresentado por Shah et al.~\cite{shah2018airsim}, com o objetivo de apoiar pesquisas em veículos autônomos, incluindo carros e VANTs. Construído sobre o motor gráfico Unreal Engine, o AirSim oferece ambientes tridimensionais de alta fidelidade visual, suporte a sensores realistas — como câmeras RGB, profundidade, sensores de distância e IMU — e integração direta com controladores de voo amplamente utilizados, como ArduPilot e PX4. Essa arquitetura permite que o simulador funcione como uma camada de abstração entre algoritmos de alto nível e controladores de baixo nível, aproximando o comportamento do sistema simulado daquele observado em operações reais.

Entre os principais pontos fortes do AirSim destacam-se sua especialização em aplicações com VANTs, a facilidade de configuração de sensores e cenários personalizados, bem como a possibilidade de realizar testes de voo autônomo baseados em scripts e missões pré-programadas. Por outro lado, o simulador apresenta limitações relacionadas ao desempenho computacional e à escalabilidade, especialmente em cenários que envolvem múltiplos agentes e demandam a execução paralela de diversos ambientes, o que pode comprometer a eficiência do treinamento de algoritmos de aprendizado por reforço multiagente.

O IsaacSim, por sua vez, é um simulador desenvolvido pela NVIDIA, fundamentado na plataforma Omniverse e apresentado como uma solução voltada à simulação robótica de alto desempenho~\cite{makoviychuk2021isaac}. Diferentemente do AirSim, o IsaacSim foi projetado desde sua concepção para oferecer escalabilidade, paralelização massiva e integração nativa com bibliotecas de aprendizado profundo. O simulador utiliza tecnologias como USD (Universal Scene Description), PhysX para simulação física e RTX para renderização acelerada por hardware, possibilitando a execução simultânea de centenas ou milhares de ambientes em GPU.

Os principais pontos fortes do IsaacSim incluem seu elevado desempenho computacional, suporte robusto à paralelização de ambientes, fidelidade física e integração direta com frameworks de aprendizado por reforço, como o IsaacLab. Essas características tornam o simulador particularmente adequado para o treinamento intensivo de políticas de controle baseadas em aprendizado por reforço, especialmente em contextos multiagentes. Em contrapartida, o IsaacSim apresenta uma curva de aprendizado mais acentuada e menor especialização nativa em aplicações aeronáuticas quando comparado ao AirSim, exigindo maior esforço na modelagem de dinâmicas específicas de VANTs.

A Figura~\ref{fig:formacao_v_airsim} ilustra um cenário de enxame de VANTs em formação em V modelado no ambiente AirSim, enquanto a Figura~\ref{fig:formacao_v_isaacsim} apresenta uma configuração equivalente implementada no IsaacSim.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/swarm_airsim.png}
    \caption{Enxame em formação em V no ambiente AirSim.}
    \label{fig:formacao_v_airsim}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/isaac-sim-env.png}
    \caption{Enxame em formação em V no ambiente IsaacSim.}
    \label{fig:formacao_v_isaacsim}
\end{figure}

No que se refere à integração com bibliotecas de aprendizado por reforço, ambos os simuladores oferecem suporte a diferentes níveis de abstração. O AirSim disponibiliza APIs em Python que permitem sua integração com frameworks como Gymnasium, Stable-Baselines e PyTorch, embora essa integração exija, em geral, a implementação manual de wrappers para adaptação ao paradigma de ambientes do tipo Markov Decision Process (MDP). Essa característica torna o AirSim mais flexível, porém menos otimizado para treinamentos em larga escala.

O IsaacSim, por outro lado, fornece integração nativa com o ecossistema de aprendizado por reforço da NVIDIA por meio do IsaacLab, que implementa interfaces compatíveis com o Gymnasium e suporta diretamente bibliotecas como PyTorch. Essa integração facilita a definição de ambientes vetorizados, o gerenciamento de observações e recompensas em larga escala, bem como a implementação de algoritmos de aprendizado por reforço multiagente de forma eficiente e modular. Em razão dessas características, o IsaacSim foi adotado como a plataforma principal para o treinamento final dos modelos e a avaliação quantitativa dos resultados apresentados neste trabalho.

% ---------------------------------------------------------
\subsection{Especificação das Tarefas dos Agentes}
\label{subsec:especificacao_tarefas}

A tarefa especificada para os agentes do enxame consistiu na navegação cooperativa em formação em V com desvio de obstáculos, em um ambiente tridimensional. O objetivo principal foi fazer com que os agentes aprendessem a se deslocar de uma posição inicial até uma posição final, mantendo a formação predefinida, evitando colisões com obstáculos estáticos e preservando uma distância segura entre si durante toda a execução da missão.

De modo geral, em ambos os simuladores, o cenário foi modelado de forma que múltiplos obstáculos fossem posicionados entre a posição inicial do enxame e a posição final a ser alcançada. A cada reinicialização do ambiente, tanto a posição inicial dos agentes quanto a posição de destino eram alteradas de forma aleatória, garantindo diversidade nos episódios de treinamento e forçando os agentes a aprenderem estratégias generalizáveis de navegação e desvio de obstáculos.

Além do desvio de obstáculos, a tarefa impôs restrições adicionais relacionadas à coesão do enxame. Os agentes deveriam manter uma distância segura entre si, evitando tanto colisões quanto uma dispersão excessiva da formação, de modo a preservar o comportamento coletivo característico de uma navegação em formação em V.
%**Melhorias
%TODO -- Adicionar informações sobre os termos, métricas e condições de término dos episódios em cada simulador.
% Explicar oque signigica um step, episodio, etc.para que o leitor compreenda melhor as condições na proxima subseção.
\subsubsection{Ambiente AirSim}

No ambiente AirSim, a tarefa foi modelada considerando o uso de controladores de voo de baixo nível, como ArduPilot e PX4, fornecidos pelo próprio simulador. Dessa forma, a política de aprendizado por reforço atuou em um nível de abstração mais elevado, sendo responsável apenas pela coordenação dos comandos enviados aos controladores de voo, sem a necessidade de aprender diretamente a estabilização do voo.

Os obstáculos presentes nesse ambiente foram modelados como cilindros em forma de pilares, distribuídos horizontalmente de modo a bloquear o sentido principal de navegação do enxame. Essa configuração exigiu que os agentes realizassem manobras de desvio enquanto mantinham a formação durante o deslocamento.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/task_obstacles_airsim.png}
    \caption{Ambiente de obstáculos para navegação em formação de enxame no AirSim.}
    \label{fig:airsim_obstaculos}
\end{figure}

O término de um episódio no AirSim ocorreu com base em um conjunto de condições relacionadas ao sucesso da missão, à segurança da formação e à limitação do tempo de execução. Essas condições estão resumidas na Tabela~\ref{tab:termino_airsim}.

\begin{table}[htbp]
\centering
\caption{Condições de término e reinicialização de episódios no AirSim.}
\label{tab:termino_airsim}
\begin{tabular}{ll}
\hline
\textbf{Condição} & \textbf{Descrição} \\
\hline
Dispersão do enxame & Distância média entre agentes maior que três vezes a distância inicial \\
Desvio excessivo do alvo & Distância ao objetivo maior que cinco vezes a distância inicial \\
Objetivo alcançado & Centroide do enxame a menos de 3 m da posição final \\
Limite de passos & Número máximo de steps do episódio (1000 steps) \\
\hline
\end{tabular}
\end{table}

A condição de dispersão do enxame foi utilizada para encerrar episódios nos quais os agentes se afastavam excessivamente da formação desejada, enquanto a condição de desvio do alvo evitou episódios longos que não apresentavam progressos significativos em direção ao objetivo. O sucesso da missão foi caracterizado pela aproximação do centroide do enxame à posição final desejada.

\subsubsection{Ambiente IsaacSim}

No ambiente IsaacSim, a tarefa foi modelada de forma distinta, uma vez que o simulador não fornece, de maneira nativa, controladores de voo equivalentes ao PX4 ou ArduPilot. Assim, os agentes precisaram aprender políticas de controle de nível mais baixo, incluindo aspectos relacionados à estabilização do voo, o que impactou diretamente as condições de término e reinicialização dos episódios.

Durante as iterações iniciais de treinamento, era comum que os agentes perdessem estabilidade e colidissem com o solo. Dessa forma, foi definida uma condição de reinicialização sempre que a altitude de um agente fosse inferior a um limite mínimo. De modo análogo, limites superiores de altitude e restrições espaciais do ambiente foram empregados para evitar comportamentos indesejados, como a saída da região válida de simulação.

As condições de término e reinicialização adotadas no IsaacSim estão resumidas na Tabela~\ref{tab:termino_isaacsim}.

\begin{table}[htbp]
\centering
\caption{Condições de término e reinicialização de episódios no IsaacSim.}
\label{tab:termino_isaacsim}
\begin{tabular}{ll}
\hline
\textbf{Condição} & \textbf{Descrição} \\
\hline
Altitude mínima & Altura do agente inferior ao limite mínimo permitido \\
Altitude máxima & Altura do agente superior ao limite máximo permitido \\
Fora dos limites & Agente fora dos limites espaciais do ambiente \\
Tempo máximo & Duração máxima do episódio excedida \\
\hline
\end{tabular}
\end{table}

Com o objetivo de facilitar o aprendizado de comportamentos complexos, foi adotada no IsaacSim uma estratégia de aprendizado curricular, na qual o treinamento foi dividido em cinco estágios progressivos. Cada estágio buscou simplificar o comportamento a ser aprendido, adicionando gradualmente novos desafios à tarefa.

\subsubsubsection{Estágios do Curriculum Learning}
%\label{subsubsec:estagios_curriculum}

O treinamento dos agentes no ambiente IsaacSim foi estruturado segundo uma estratégia de aprendizado curricular (\textit{Curriculum Learning}), na qual a tarefa global de navegação cooperativa em formação com desvio de obstáculos foi decomposta em cinco estágios progressivos. Cada estágio foi projetado para enfatizar um subconjunto específico de habilidades, permitindo que os agentes adquirissem comportamentos básicos antes de serem expostos a desafios mais complexos. A Figura~\ref{fig:curriculum_overview} apresenta uma visão geral dos cenários adotados em cada etapa do currículo.

No primeiro estágio, denominado \textit{Hover} (Figura~\ref{fig:curriculum_stage1}), os drones iniciam o episódio com suas posições organizadas em forma de grid no plano \(xy\). As posições finais apresentam variação mínima nas coordenadas \(x\) e \(y\), enquanto a coordenada \(z\) é definida dentro de um intervalo configurável para cada agente. Essa configuração tem como objetivo enfatizar o aprendizado do controle de altitude, induzindo os agentes a aumentarem sua altura com deslocamento mínimo no plano horizontal. As posições finais desejadas são indicadas visualmente pelos pontos verdes, reforçando que o principal comportamento esperado nesse estágio é a ascensão vertical e a estabilização do voo.

No segundo estágio, ilustrado na Figura~\ref{fig:curriculum_stage2}, a tarefa consiste na navegação ponto a ponto no plano \(xy\), sem a consideração de comportamentos de enxame. Cada drone é alocado em uma “camada” distinta do espaço tridimensional, caracterizada por uma altura \(z\) fixa, com um deslocamento \(\Delta z\) entre os agentes. As posições finais mantêm a mesma coordenada \(z\) da posição inicial, sendo indicadas pelos pontos verdes, enquanto as coordenadas \(x\) e \(y\) são transladadas. Essa configuração força cada agente a aprender a se movimentar no plano horizontal correspondente à sua camada, preservando a altitude constante, o que contribui para o refinamento das habilidades de navegação planar.

O terceiro estágio, apresentado na Figura~\ref{fig:curriculum_stage3}, introduz obstáculos estáticos posicionados entre a posição inicial e a posição objetivo de cada drone. Nesse cenário, o objetivo principal é enfatizar o comportamento de desvio de obstáculos, induzindo os agentes a realizarem trajetórias do tipo zig-zag para contornar as barreiras. Neste estágio, os agentes ainda não consideram restrições relacionadas à distância entre si nem à manutenção de formação, concentrando-se exclusivamente na adaptação da trajetória para evitar colisões com os obstáculos. As posições finais continuam sendo indicadas pelos pontos verdes.

No quarto estágio, ilustrado na Figura~\ref{fig:curriculum_stage4}, o comportamento de enxame passa a ser explicitamente considerado. Os agentes iniciam o episódio organizados em uma formação em V, e as posições finais são definidas por meio de translações e rotações aplicadas à configuração inicial da formação. Essa característica força os agentes a aprenderem a navegar cooperativamente, preservando a geometria da formação e evitando colisões entre si. Nesse cenário, o marcador visual em azul representa o centroide do enxame, enquanto as posições finais individuais dos agentes são indicadas pelos pontos amarelos.

Por fim, o quinto estágio, representado na Figura~\ref{fig:curriculum_stage5}, corresponde ao cenário mais complexo do currículo. Nesse estágio, são adicionados obstáculos entre a posição inicial do enxame e a posição final desejada. Diferentemente dos estágios anteriores, a posição objetivo é definida em relação ao centroide do enxame, indicado visualmente por um cubo verde posicionado após a região de obstáculos. Esse arranjo reforça a necessidade de os agentes considerarem simultaneamente múltiplos fatores, incluindo a distância entre si, a proximidade dos obstáculos e a navegação coordenada em direção à posição final coletiva.

\begin{figure}[H]
    \centering

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/stage1_example.png}
        \caption{Estágio 1: Hover.}
        \label{fig:curriculum_stage1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/stage2_example.png}
        \caption{Estágio 2: Navegação ponto a ponto em camadas de altitude.}
        \label{fig:curriculum_stage2}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/stage3_example.png}
        \caption{Estágio 3: Navegação com desvio de obstáculos.}
        \label{fig:curriculum_stage3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/stage4_example.png}
        \caption{Estágio 4: Navegação cooperativa em formação em V.}
        \label{fig:curriculum_stage4}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/stage5_example.png}
        \caption{Estágio 5: Navegação em formação com obstáculos.}
        \label{fig:curriculum_stage5}
    \end{subfigure}

    \caption{Visão geral dos estágios do aprendizado curricular adotado no IsaacSim.}
    \label{fig:curriculum_overview}
\end{figure}

Essa estratégia de aprendizado curricular permitiu uma progressão gradual da complexidade da tarefa, contribuindo para a estabilidade do treinamento e para a aquisição de comportamentos cooperativos cada vez mais sofisticados. Os impactos dessa abordagem sobre a eficiência do aprendizado e o desempenho final dos agentes serão analisados nas seções subsequentes de treinamento e resultados.


% ---------------------------------------------------------

\subsection{Espaço de Estados e Ações}
\label{subsec:espaco_estados_acoes}

A definição do espaço de estados (ou observações) e do espaço de ações constitui um dos elementos centrais na formulação de problemas de aprendizado por reforço, pois estabelece a interface entre a percepção do agente e sua capacidade de atuação no ambiente. No contexto deste trabalho, esses espaços foram projetados de modo a refletir informações que seriam, em princípio, acessíveis a um VANT real por meio de sensores embarcados e atuadores físicos, garantindo coerência entre a modelagem em simulação e uma possível aplicação em cenários reais.

Cada agente observa apenas informações locais e parciais do ambiente, caracterizando um problema de decisão sob observabilidade parcial. Em contrapartida, durante a fase de treinamento centralizado, informações adicionais de caráter global podem ser exploradas pelo crítico, conforme o paradigma de \textit{Centralized Training with Decentralized Execution} (CTDE). Essa distinção permite que o aprendizado seja auxiliado por um ``oráculo'' durante o treinamento, enquanto a política aprendida permanece executável de forma descentralizada, utilizando exclusivamente observações locais.

\subsubsection{Espaço de Observação}

De forma geral, o vetor de observação de cada agente é composto por informações proprioceptivas, informações relativas à tarefa e percepções do ambiente. As informações proprioceptivas incluem estados cinemáticos do próprio agente, como posição, velocidade e orientação. As informações relativas à tarefa abrangem medidas associadas à posição objetivo, permitindo que o agente avalie seu progresso em relação à missão. Por fim, as percepções do ambiente são obtidas a partir de sensores simulados, como sensores de distância, raycasts ou câmeras, utilizados para detectar obstáculos e a presença de outros agentes no entorno.

No contexto multiagente, cada agente possui acesso apenas a observações locais de seus vizinhos mais próximos, como posições ou velocidades relativas, reforçando a natureza descentralizada do problema. Informações globais, como o estado completo do enxame ou o centroide global, são utilizadas exclusivamente pelo crítico durante o treinamento e não estão disponíveis para a política durante a execução.

A Figura~\ref{fig:obs_action_space} ilustra de forma conceitual o espaço de observação e o espaço de ações de um agente do enxame.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/uav_perception.jpeg}
    \caption{Ilustração conceitual do espaço de observação e do espaço de ações de um agente do enxame.}
    \label{fig:obs_action_space}
\end{figure}

\subsubsection{Espaço de Ações}

O espaço de ações adotado neste trabalho é contínuo, refletindo a natureza do controle de VANTs. As ações correspondem a comandos que influenciam diretamente a dinâmica do agente, podendo representar velocidades desejadas, acelerações, empuxos ou momentos, a depender do nível de abstração adotado pelo simulador. Essa modelagem permite a aplicação de algoritmos de aprendizado por reforço contínuo e a representação adequada de comportamentos de navegação e controle.

\subsubsection{Instanciação no Ambiente AirSim}

No ambiente AirSim, o espaço de observação de cada agente é multimodal, combinando informações visuais e vetoriais provenientes de diferentes sensores simulados. Para um agente $i$ no instante $t$, a observação local é definida como:
\[
\mathbf{o}_i^t =
\big[
\mathbf{I}_i^t,
\mathbf{p}_i^t,
\boldsymbol{\theta}_i^t,
\mathbf{v}_i^t,
d_{i,g}^t,
d_{i,o}^t,
\mathbf{d}_{i,\mathcal{N}}^t
\big].
\]

Cada componente do vetor de observação possui significado físico e dimensionalidade bem definida, conforme descrito a seguir:

\begin{itemize}
    \item $\mathbf{I}_i^t \in \mathbb{R}^{64 \times 64 \times 1}$: imagem de profundidade capturada pelo agente $i$, utilizada para a percepção espacial do ambiente e detecção de obstáculos à frente do drone.
    
    \item $\mathbf{p}_i^t \in \mathbb{R}^{3}$: vetor de posição tridimensional do agente, expresso no referencial global do ambiente, representando as coordenadas $(x, y, z)$ em metros.
    
    \item $\boldsymbol{\theta}_i^t \in \mathbb{R}^{3}$: vetor de orientação do agente, composto pelos ângulos de guinada (\textit{yaw}), arfagem (\textit{pitch}) e rolagem (\textit{roll}), em radianos.
    
    \item $\mathbf{v}_i^t \in \mathbb{R}^{3}$: vetor de velocidade linear do agente, representando as componentes $(v_x, v_y, v_z)$ no espaço tridimensional, em metros por segundo.
    
    \item $d_{i,g}^t \in \mathbb{R}$: distância escalar entre o agente $i$ e a posição objetivo, medida em metros, utilizada para avaliar o progresso da navegação em direção ao destino.
    
    \item $d_{i,o}^t \in \mathbb{R}$: distância frontal ao obstáculo mais próximo detectado pelo agente, medida em metros, fornecida por sensores de distância simulados.
    
    \item $\mathbf{d}_{i,\mathcal{N}}^t \in \mathbb{R}^{N \times N}$: matriz de distâncias interagentes, onde cada elemento representa a distância euclidiana, em metros, entre pares de agentes do enxame.
\end{itemize}

Durante o treinamento centralizado, o crítico tem acesso a uma observação global contendo informações completas do enxame, incluindo estados e relações espaciais entre todos os agentes. Essas informações não estão disponíveis durante a execução descentralizada das políticas, caracterizando explicitamente um problema sob observabilidade parcial, conforme o paradigma de \textit{Centralized Training with Decentralized Execution} (CTDE).

O espaço de ações no AirSim é contínuo e definido, para cada agente, como:
\[
\mathbf{a}_i^t \in [-1,1]^4,
\]
onde os elementos do vetor de ação correspondem a comandos normalizados de movimento linear e taxa de guinada. Esses comandos são posteriormente interpretados pelos controladores de voo embarcados do simulador, como PX4 ou ArduPilot, de modo que a política aprende um controle de alto nível, enquanto a estabilização do voo é delegada ao controlador de baixo nível.

\subsubsection{Instanciação no Ambiente IsaacSim}

No ambiente IsaacSim, o espaço de observação adotado para cada agente é definido como um vetor contínuo de dimensão fixa. Diferentemente do AirSim, não são utilizadas observações visuais, sendo o estado completamente descrito por informações vetoriais diretamente relacionadas à dinâmica do VANT, à tarefa e às interações locais com o ambiente e com outros agentes. Para um agente $i$ no instante $t$, a observação individual é dada por:
\[
\mathbf{o}_i^t \in \mathbb{R}^{23}.
\]

Esse vetor é obtido pela concatenação dos seguintes componentes, cada um com significado físico e dimensionalidade bem definida:
\[
\mathbf{o}_i^t =
\big[
\mathbf{v}_i^t,
\boldsymbol{\omega}_i^t,
\mathbf{g}_i^t,
\mathbf{p}_{i,g}^t,
d_{i,o}^t,
\Delta \mathbf{v}_{i,\mathcal{N}}^t,
\Delta \mathbf{p}_{i,\mathcal{N}}^t,
\mathbf{q}_i^t
\big].
\]

A descrição detalhada de cada termo é apresentada a seguir:

\begin{itemize}
    \item $\mathbf{v}_i^t \in \mathbb{R}^{3}$: vetor de velocidade linear do agente, expresso no referencial do corpo, representando as componentes $(v_x, v_y, v_z)$ em metros por segundo.
    
    \item $\boldsymbol{\omega}_i^t \in \mathbb{R}^{3}$: vetor de velocidades angulares do agente, correspondente às taxas de rolagem, arfagem e guinada $(\omega_x, \omega_y, \omega_z)$, em radianos por segundo.
    
    \item $\mathbf{g}_i^t \in \mathbb{R}^{3}$: vetor da gravidade projetado no referencial do corpo do drone, utilizado como referência para orientação e estabilização do voo.
    
    \item $\mathbf{p}_{i,g}^t \in \mathbb{R}^{3}$: vetor posição relativa entre o agente $i$ e seu objetivo individual, expresso no referencial do corpo, indicando a direção e magnitude do deslocamento necessário para que o agente avance em direção à sua meta designada.
    
    \item $d_{i,o}^t \in \mathbb{R}$: distância escalar ao obstáculo mais próximo, medida em metros, obtida a partir de sensores de distância simulados.
    
    \item $\Delta \mathbf{v}_{i,\mathcal{N}}^t \in \mathbb{R}^{3}$: vetor de velocidade relativa entre o agente $i$ e seu vizinho mais próximo, utilizado para modelar interações locais e auxiliar na prevenção de colisões.
    
    \item $\Delta \mathbf{p}_{i,\mathcal{N}}^t \in \mathbb{R}^{3}$: vetor de posição relativa entre o agente $i$ e seu vizinho mais próximo, expresso no referencial do corpo, permitindo que o agente avalie sua proximidade em relação aos demais membros do enxame.
    
    \item $\mathbf{q}_i^t \in \{0,1\}^{4}$: vetor \textit{one-hot} que representa o estado corrente da Reward Machine associada ao agente, codificando o progresso da tarefa dentro da estratégia de aprendizado curricular.
\end{itemize}

Durante o treinamento centralizado, o crítico tem acesso ao estado global do sistema, definido como a concatenação direta das observações individuais de todos os agentes:
\[
\mathbf{s}^t =
\big[
\mathbf{o}_1^t,
\mathbf{o}_2^t,
\dots,
\mathbf{o}_N^t
\big]
\in \mathbb{R}^{23N}.
\]
Esse estado global não está disponível durante a execução descentralizada das políticas, caracterizando explicitamente um problema sob observabilidade parcial e mantendo consistência com o paradigma de \textit{Centralized Training with Decentralized Execution} (CTDE). Ressalta-se que informações relacionadas ao centroide do enxame são utilizadas apenas em condições de término dos episódios e não fazem parte do espaço de observação dos agentes.

O espaço de ações no IsaacSim também é contínuo e definido, para cada agente, como:
\[
\mathbf{a}_i^t \in [-1,1]^4,
\]
onde os elementos do vetor de ação correspondem a comandos normalizados de empuxo e momentos aplicados diretamente ao modelo dinâmico do VANT. Diferentemente do ambiente AirSim, não há uma camada intermediária de controle baseada em controladores de voo embarcados, o que implica que a política aprendida é responsável por aspectos de controle mais próximos do nível físico, incluindo a estabilização do voo.

\subsubsection{Síntese Comparativa}

Em síntese, enquanto o AirSim favorece uma modelagem baseada em observações multimodais e ações de alto nível, suportadas por controladores de voo embarcados, o IsaacSim adota uma representação mais compacta do espaço de observação e ações aplicadas diretamente à dinâmica do agente. Essas diferenças motivaram escolhas distintas de arquiteturas neurais e estratégias de treinamento, conforme discutido nas seções subsequentes, sem comprometer a coerência conceitual da tarefa de navegação cooperativa em formação.


% ---------------------------------------------------------
\subsection{Funções de Recompensa}
\label{subsec:funcoes_recompensa}

% Formulação matemática das recompensas
% Termos individuais
% Termos coletivos
% Penalizações
% Reward Machines
% Estados, transições e eventos
% Integração das RM ao processo de aprendizado multiagente

% =========================================================
\section{Algoritmo de Aprendizado}
\label{sec:algoritmo_aprendizado}

Esta seção descreve o algoritmo de aprendizado por reforço multiagente utilizado, bem como sua implementação computacional nos diferentes ambientes de simulação.

% ---------------------------------------------------------
\subsection{MAPPO: Formulação e Pseudocódigo}
\label{subsec:mappo_pseudocodigo}

% Centralized Training with Decentralized Execution (CTDE)
% Formulação do MAPPO
% Função objetivo
% Atualização das políticas
% Apresentação do pseudocódigo comentado

% ---------------------------------------------------------
\subsection{Implementação Computacional}
\label{subsec:implementacao}

% Implementação no TorchRL
% Implementação no IsaacLab
% Arquiteturas neurais adotadas
% Attention + CNN no AirSim
% MLP no IsaacSim
% Justificativas das escolhas arquiteturais

% =========================================================
\section{Estratégias de Treinamento}
\label{sec:estrategias_treinamento}

Esta seção apresenta as estratégias de treinamento adotadas, incluindo a comparação entre simuladores e as abordagens baseadas em aprendizado curricular com e sem o uso de Reward Machines.

% ---------------------------------------------------------
\subsection{Treinamento no AirSim}
\label{subsec:treinamento_airsim}

% Objetivo exploratório
% Limitações computacionais
% Configuração dos experimentos
% Observações iniciais

% ---------------------------------------------------------
\subsection{Treinamento no IsaacLab}
\label{subsec:treinamento_isaaclab}

% Configuração final de treinamento
% Paralelização
% Estabilidade e escalabilidade
% Vantagens observadas

% ---------------------------------------------------------
\subsection{Comparação de Performance entre Simuladores}
\label{subsec:comparacao_simuladores}

% Comparação qualitativa
% Comparação quantitativa
% Impacto no tempo de treinamento
% Discussão sobre fidelidade física

% ---------------------------------------------------------
\subsection{Abordagem Baseline}
\label{subsec:baseline}

% Curriculum Learning sem Reward Machines
% Definição dos estágios
% Objetivo da baseline

% ---------------------------------------------------------
\subsection{Curriculum Learning com Reward Machines}
\label{subsec:curriculum_rm}

% Integração entre Curriculum Learning e RM
% Definição formal dos estágios:
% 1. Hover
% 2. Point-to-Point
% 3. Desvio de obstáculos (single-agent)
% 4. Navegação em formação em V
% 5. Navegação em enxame com obstáculos
% Discussão da progressão entre estágios
