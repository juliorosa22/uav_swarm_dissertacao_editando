\section{Treinamento dos Agentes}
\label{sec:treinamento_dos_agentes}

Esta seção descreve as estratégias de treinamento adotadas para o aprendizado das políticas de navegação cooperativa em enxames de VANTs, considerando os diferentes ambientes de simulação utilizados ao longo do trabalho. São apresentados os procedimentos de treinamento empregados no AirSim e no IsaacSim, bem como as principais decisões metodológicas relacionadas à configuração dos experimentos, ao uso (ou não) de aprendizado curricular e à incorporação de Máquinas de Recompensa.

Todos os experimentos foram conduzidos em uma estação de trabalho dedicada, equipada com uma GPU NVIDIA RTX 4070 com 12\,GB de memória, 62\,GB de memória RAM e processador Intel Core i9-12900 (12ª geração, 24 núcleos lógicos). Essa configuração foi suficiente para suportar tanto o treinamento em ambientes com simulação física mais custosa quanto a execução paralela de múltiplos ambientes no caso do IsaacSim.

\subsection{Treinamento no AirSim}
\label{subsec:treinamento_airsim}

No ambiente AirSim, o treinamento dos agentes foi conduzido de forma direta, sem a adoção de aprendizado curricular ou de Máquinas de Recompensa, com o objetivo de estabelecer uma linha de base (\textit{baseline}) para a avaliação do algoritmo MAPPO em um cenário com maior nível de abstração de controle. Nesse contexto, os agentes interagem com o ambiente por meio de comandos de alto nível, os quais são interpretados por controladores de voo embarcados.

%O cenário de treinamento considerou um enxame composto por cinco agentes, mantendo fixa a cardinalidade do grupo ao longo de todo o processo de aprendizado. O treinamento totalizou aproximadamente $2 \times 10^{6}$ passos de interação com o ambiente, realizados em um único cenário de simulação, sem paralelização de ambientes. Essa configuração decorreu das limitações computacionais associadas ao custo do passo de simulação no AirSim, significativamente superior ao observado em simuladores que suportam execução massivamente paralela.

O principal gargalo identificado durante o treinamento esteve relacionado à etapa de coleta de experiências, uma vez que o tempo necessário para a execução de cada passo de simulação impactou diretamente a taxa de amostragem do algoritmo MAPPO. Como consequência, observou-se menor eficiência amostral e maior sensibilidade a instabilidades no processo de aprendizado, em particular na atualização da função de valor estimada pelo crítico.

Apesar dessas limitações, os experimentos conduzidos no AirSim desempenharam um papel fundamental na validação inicial da modelagem do problema, da definição dos espaços de observação e ação e do projeto da função de recompensa monolítica. Os resultados obtidos nessa etapa serviram como referência comparativa para os experimentos realizados posteriormente no ambiente IsaacSim, nos quais foram introduzidas estratégias de aprendizado curricular, paralelização massiva de ambientes e o uso de Máquinas de Recompensa.


% ---------------------------------------------------------
\subsection{Treinamento no IsaacLab}
\label{subsec:treinamento_isaaclab}


O treinamento dos agentes no ambiente IsaacLab foi estruturado a partir de uma estratégia de aprendizado curricular, na qual a complexidade das tarefas aumenta progressivamente ao longo de diferentes estágios. Essa abordagem visa decompor uma missão cooperativa complexa em subtarefas mais simples, permitindo uma avaliação controlada do impacto da dificuldade do ambiente sobre o processo de aprendizado. Em todos os experimentos, explorou-se a capacidade de paralelização massiva do simulador, com a execução simultânea de até 2048 ambientes paralelos, contribuindo para maior eficiência amostral no treinamento do algoritmo MAPPO.

Embora a estratégia inicialmente prevista contemplasse a transferência de aprendizado entre estágios consecutivos, por meio da inicialização das políticas de um estágio a partir dos pesos finais do estágio anterior, dificuldades técnicas associadas à inicialização consistente dos modelos impediram a adoção desse procedimento dentro do prazo disponível para a execução do trabalho. Em função dessas limitações, optou-se por treinar cada estágio de forma independente, mantendo-se, contudo, a progressão incremental de complexidade das tarefas como princípio organizador do currículo.

% Os cinco estágios considerados neste trabalho foram definidos da seguinte forma: 
% \begin{itemize}
%     \item \textbf{Estágio 1 - Estabilização em Altitude (\textit{Hover})} 
%     \item \textbf{Estágio 2 - Navegação Ponto a Ponto Individual}
%     \item \textbf{Estágio 3 - Navegação com Desvio de Obstáculos sem Formação}.
%     \item \textbf{Estágio 4 - Navegação Ponto a Ponto em Formação de Enxame}.
%     \item \textbf{Estágio 5 - Navegação Cooperativa em Formação com Desvio de Obstáculos} 
% \end{itemize}
%(i) estabilização em altitude (\textit{hover}); (ii) navegação ponto a ponto individual; (iii) navegação com desvio de obstáculos sem formação; (iv) navegação ponto a ponto em formação de enxame; e (v) navegação cooperativa em formação com desvio de obstáculos. Cada estágio foi treinado a partir de uma inicialização independente dos parâmetros da política, com o número total de interações com o ambiente ajustado de acordo com a complexidade da tarefa. %Especificamente, foram utilizados aproximadamente 200 mil timesteps no Estágio~1, 250 mil no Estágio~2, 300 mil no Estágio~3, 400 mil no Estágio~4 e 1 milhão de timesteps no Estágio~5.

Adicionalmente, com o objetivo de realizar uma análise de ablação, foi conduzido um experimento de treinamento adicional no Estágio~5 utilizando uma abordagem baseline, na qual não se empregaram Máquinas de Recompensa. Os resultados dessa comparação são discutidos em detalhes no Capítulo~5.

%Em todos os estágios treinados com Máquinas de Recompensa, a RM permaneceu ativa durante todo o processo de aprendizado, sendo responsável por modular dinamicamente a função de recompensa em função do estado lógico corrente de cada agente. Dessa forma, enquanto a separação em estágios define a progressão global da complexidade do problema, a RM atua em um nível mais fino de abstração, orientando o comportamento dos agentes por meio da decomposição estruturada da tarefa. Essa combinação mostrou-se fundamental para viabilizar o aprendizado em cenários cooperativos de crescente complexidade, mesmo na ausência de transferência direta de parâmetros entre estágios.

A Figura~\ref{fig:curriculum_training_isaaclab} apresenta uma visão esquemática da organização dos estágios de treinamento adotados no ambiente IsaacLab.

\begin{figure}[H]
\centering
\scalebox{0.8}{% Scale to 80%
\begin{tikzpicture}[
    node distance=1cm,
    stage/.style={
        rectangle,
        draw,
        rounded corners,
        minimum width=4.6cm,
        minimum height=1.2cm,
        align=center,
        font=\small
    },
    arrow/.style={->, thick}
]

% =======================
% Stages (Top -> Bottom)
% =======================
\node[stage] (s1) {
    \textbf{Estágio 1}\\
    Hovering\\
    Política $\pi_{1}$
};

\node[stage, below=of s1] (s2) {
    \textbf{Estágio 2}\\
    Navegação ponto a ponto\\
    Política $\pi_{2}$
};

\node[stage, below=of s2] (s3) {
    \textbf{Estágio 3}\\
    Navegação com obstáculos\\
    (sem formação)\\
    Política $\pi_{3}$
};

\node[stage, below=of s3] (s4) {
    \textbf{Estágio 4}\\
    Navegação em formação\\
    (sem obstáculos)\\
    Política $\pi_{4}$
};

\node[stage, below=of s4] (s5) {
    \textbf{Estágio 5}\\
    Formação com obstáculos\\
    (cenário final)\\
    Política $\pi_{5}$
};

% =======================
% Arrows (complexity progression)
% =======================
\draw[arrow] (s1) -- node[right, font=\scriptsize]{aumento da complexidade} (s2);
\draw[arrow] (s2) -- node[right, font=\scriptsize]{aumento da complexidade} (s3);
\draw[arrow] (s3) -- node[right, font=\scriptsize]{aumento da complexidade} (s4);
\draw[arrow] (s4) -- node[right, font=\scriptsize]{aumento da complexidade} (s5);

\end{tikzpicture}
}% End scalebox
\caption{Organização dos estágios de treinamento adotados no ambiente IsaacLab.}
% O processo é estruturado em cinco estágios de complexidade crescente, os quais definem um currículo conceitual para a tarefa de navegação cooperativa. Cada estágio é treinado de forma independente, resultando em políticas distintas ($\pi_{1}$ a $\pi_{5}$), sem transferência direta de parâmetros entre estágios. Em todos os casos, a Máquina de Recompensa permanece ativa durante o treinamento, modulando dinamicamente a função de recompensa de acordo com o estado lógico dos agentes.
\label{fig:curriculum_training_isaaclab}
\end{figure}



% ---------------------------------------------------------
\subsection{Comparação de Performance entre Simuladores}
\label{subsec:comparacao_simuladores}
Esta seção apresenta uma análise comparativa do desempenho do algoritmo MAPPO quando treinado nos ambientes AirSim e IsaacSim. O objetivo principal é justificar, de forma quantitativa e qualitativa, a decisão de substituir o AirSim pelo IsaacSim como principal plataforma de simulação ao longo do desenvolvimento deste trabalho. A análise concentra-se no comportamento das funções de custo da política (\textit{policy loss}) e da rede crítica (\textit{value loss}), por serem indicadores diretos de estabilidade e eficiência do processo de aprendizado.

\subsubsection*{\textbf{Análise da \textit{Policy Loss}}}

A Figura~\ref{fig:airsim_loss_policy_large} apresenta a evolução da \textit{policy loss} durante o treinamento no ambiente AirSim. O processo foi conduzido por aproximadamente $3.5 \times 10^{5}$ iterações, distribuídas ao longo de cerca de quatro dias de execução contínua. Observa-se um comportamento fortemente oscilatório, sem tendência clara de convergência, indicando instabilidade nos gradientes e dificuldade da política em aprender comportamentos consistentes.

Esse resultado está diretamente associado a limitações estruturais do AirSim, em especial à baixa capacidade de paralelização e ao elevado custo computacional de cada passo de simulação. Além disso, o treinamento foi realizado sem o suporte de aprendizado curricular, exigindo que a política aprendesse simultaneamente tarefas de estabilização, navegação e desvio de obstáculos, o que contribuiu para a elevada variância das atualizações.

Em contraste, a Figura~\ref{fig:isaacsim_loss_policy_large} apresenta o comportamento da \textit{policy loss} no ambiente IsaacSim. Em aproximadamente $3 \times 10^{4}$ iterações — correspondendo a cerca de 20 minutos de treinamento — observa-se um decaimento monotônico da função de custo até valores próximos de zero. Esse comportamento indica estabilidade do gradiente, rápida convergência e um processo de aprendizado significativamente mais eficiente.

O ganho observado está diretamente relacionado à capacidade do IsaacSim de executar milhares de ambientes em paralelo na GPU, permitindo uma coleta massiva de experiências e reduzindo a correlação temporal entre amostras. Esses resultados evidenciam uma melhoria substancial na eficiência do treinamento ao migrar para o IsaacSim.


\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/loss_policy_airsim.png}
    \caption{AirSim — Função de custo da política ao longo do treinamento}
    \label{fig:airsim_loss_policy_large}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/isaacsim_loss_policy.png}
    \caption{IsaacSim — Função de custo da política ao longo do treinamento}
    \label{fig:isaacsim_loss_policy_large}
\end{figure}



% ---------------------------------------------------------

\subsubsection*{\textbf{Análise da \textit{Value Loss}}}

A Figura~\ref{fig:airsim_loss_value_large} apresenta o comportamento da \textit{value loss} durante o treinamento no ambiente AirSim. Após um decaimento inicial, observa-se que a função de custo da rede crítica passa a oscilar em torno de valores elevados, da ordem de $10^{4}$, indicando instabilidade na estimação da função de valor. Tal comportamento compromete a qualidade dos gradientes utilizados na atualização da política, afetando negativamente o desempenho global do algoritmo MAPPO.

Além das limitações inerentes ao simulador, um fator adicional que contribuiu para esse comportamento foi a arquitetura adotada para a rede crítica no AirSim. Conforme discutido na Seção~\ref{subsubsec:airsim_redes}, o crítico foi projetado com um número reduzido de entradas, priorizando uma representação mais compacta do estado global do enxame e não utilizando diretamente informações visuais, como imagens de profundidade. Embora essa escolha tenha como motivação favorecer generalização e reduzir o número de parâmetros, ela se mostrou insuficiente para capturar a complexidade do ambiente e das interações multiagentes, resultando em uma capacidade limitada de aproximação da função de valor.

Em contraste, a abordagem adotada no IsaacSim utilizou uma arquitetura de crítico mais expressiva, baseada na concatenação das observações individuais dos agentes, o que forneceu uma representação global mais rica do estado do sistema. Como consequência, conforme ilustrado na Figura~\ref{fig:isaacsim_loss_value_large}, a \textit{value loss} apresenta um decaimento suave e consistente, atingindo valores próximos de zero em aproximadamente $3 \times 10^{4}$ iterações. Esse comportamento indica que a rede crítica foi capaz de estimar de forma mais precisa a função de valor, fornecendo sinais de aprendizado mais estáveis para a política.

Embora seja possível atribuir parte da instabilidade observada no AirSim às escolhas arquiteturais do crítico, o cronograma do projeto e a elevada complexidade computacional do simulador tornaram inviável a realização de múltiplas iterações de refinamento arquitetural nesse ambiente. Dessa forma, a migração para o IsaacSim mostrou-se necessária, permitindo explorar arquiteturas mais expressivas, maior paralelização e obtenção mais rápida de métricas confiáveis de desempenho, aspectos fundamentais para a continuidade e consolidação do trabalho.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/loss_value_airsim.png}
    \caption{AirSim — Função de custo da rede crítica ao longo do treinamento}
    \label{fig:airsim_loss_value_large}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/isaacsim_loss_value.png}
    \caption{IsaacSim — Função de custo da rede crítica ao longo do treinamento}
    \label{fig:isaacsim_loss_value_large}
\end{figure}


\subsection{Discussão}

A comparação entre os resultados iniciais obtidos nos ambientes AirSim e IsaacSim evidencia diferenças expressivas em termos de estabilidade, velocidade de convergência e eficiência computacional. Enquanto o AirSim apresentou treinamento lento, ruidoso e com métricas de custo instáveis, o IsaacSim demonstrou desempenho significativamente superior, com rápida convergência e comportamento consistente das funções de custo.

Esses resultados justificam de forma objetiva a migração da stack de simulação adotada neste trabalho. A capacidade de paralelização em GPU, aliada à integração nativa com frameworks modernos de aprendizado por reforço, torna o IsaacSim mais adequado para o treinamento de políticas multiagentes complexas em cenários de enxames de VANTs.

Dessa forma, os experimentos subsequentes e as análises finais deste trabalho foram conduzidos prioritariamente no ambiente IsaacSim.
