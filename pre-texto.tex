% ---
% Dedicatória
% ---
% \begin{dedicatoria}
%    \vspace*{\fill}
%    \centering
%    \noindent
%    \textit{\textcolor{red}{EM ELABORAÇÃO}} \vspace*{\fill}
% \end{dedicatoria}
% ---

% ---
% Agradecimentos
% ---
% \begin{agradecimentos}
% \textit{\textcolor{red}{EM ELABORAÇÃO}}
% \end{agradecimentos}
% ---

% ---
% Epígrafe
% ---
% \begin{epigrafe}
%     \vspace*{\fill}
% 	\begin{flushright}
% 		\textit{\textcolor{red}{EM ELABORAÇÃO}}
% 	\end{flushright}
% \end{epigrafe}
% ---

% ---
% RESUMOS
% ---

% resumo em português
\setlength{\absparsep}{18pt} % ajusta o espaçamento dos parágrafos do resumo
\begin{resumo}
\SingleSpacing
Este trabalho propõe um novo framework que integra Máquinas de Recompensa (Reward Machines - RMs) com algoritmos de Aprendizado por Reforço Multiagente (Multi-Agent Reinforcement Learning - MARL) denomidado RM-MARL, visando ao desenvolvimento de agentes autônomos para enxames de Veículos Aéros Não Tripulados (VANTs).
A utilização das Máquinas de Recompensa possibilita a exposição da estrutura interna da função recompensa durante o treinamento dos agentes, permitindo a convergência para políticas ótimas de maneira mais eficiente e reduzindo o tempo de treinamento. Ao permitir a especificação estruturada e hierárquica de uma função recompensa por meio das RMs, o framework proposto busca capacitar agentes descentralizados a aprenderem comportamentos complexos compostos por múltiplas subtarefas dentro de uma mesma missão. O resultado esperado é um framework reutilizável para o desenvolvimento de enxames autônomos voltados a missões complexas. 
%A arquitetura segue o paradigma de Treinamento Descentralizado e Execução Descentralizada (DTDE) e foi projetada para ser reutilizável em diferentes cenários de missões com enxames. Os algoritmos IPPO e QMIX são utilizados como base de aprendizado, e a validação será realizada em uma missão de rastreamento de alvos, com o intuito de demonstrar a capacidade do sistema em guiar os agentes por etapas como detecção, aproximação e rastreamento contínuo de um alvo móvel.


\vspace{\onelineskip}

\noindent
\textbf{Palavras-chave}: \imprimirpalavraschave
\end{resumo}

% resumo em inglês
\begin{resumo}[Abstract]
\begin{otherlanguage*}{english}
%  \linespread{1.3}
\SingleSpacing
This work proposes a novel framework that integrates Reward Machines (RMs) with Multi-Agent Reinforcement Learning (MARL) algorithms, aiming to develop autonomous agents for UAV swarms.
The use of Reward Machines allows the internal structure of the reward function to be explicitly represented during agent training, enabling more efficient convergence toward optimal policies and reducing training time.
By allowing a structured and hierarchical specification of the reward function through RMs, the proposed framework seeks to enable decentralized agents to learn complex behaviors composed of multiple subtasks within a single mission.
The expected outcome is a reusable framework for the development of autonomous swarms capable of handling complex missions.
%The architecture follows the Decentralized Training and Decentralized Execution (DTDE) paradigm and is designed to be reusable across different swarm mission scenarios.

%IPPO and QMIX algorithms are used as the learning backbone, and validation will be conducted in a target-tracking mission to demonstrate the system's ability to guide agents through stages such as detection, approach, and continuous tracking of a moving target.


\vspace{\onelineskip}

\noindent 
\textbf{Keywords}: \imprimirkeywords
\end{otherlanguage*}
\end{resumo}