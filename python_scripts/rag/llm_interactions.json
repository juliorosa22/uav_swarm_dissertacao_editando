{
  "interactions": [
    {
      "timestamp": "2025-12-23T19:43:30.611547",
      "question": "\n    Based on the reviewed works in the provided context, classify each study according\nto the level at which artificial intelligence is applied:\nlow-level control (LL), mid-level control (ML), or high-level decision-making (HL).\n\nFor each classification, briefly justify the choice based on the described\naction space and system architecture.\n\nReturn the result as a structured list suitable for conversion into a LaTeX table.\n",
      "answer": "Aqui está a classificação dos estudos revisados de acordo com o nível em que a inteligência artificial é aplicada, juntamente com as justificativas:\n\n\\begin{itemize}\n    \\item **Chen et al. (2024)** - **Controle de Nível Baixo (LL)**\n    \\begin{itemize}\n        \\item Justificativa: O estudo descreve uma arquitetura de software dividida em três camadas, onde a última camada é responsável pela execução de ações complexas de voo de veículos aéreos não tripulados (VANTs) e pela obtenção de dados de vídeo e sensores. Isso indica um foco em controle direto e execução de tarefas específicas, característico do controle de nível baixo.\n    \\end{itemize}\n    \n    \\item **Xiao et al. (2024)** - **Controle de Nível Médio (ML)**\n    \\begin{itemize}\n        \\item Justificativa: O algoritmo proposto aborda a cobertura dinâmica de áreas por múltiplos VANTs, o que implica em decisões sobre a alocação de recursos e a adaptação a áreas de interesse dinâmicas. Isso sugere um nível de controle que vai além da execução de tarefas específicas, envolvendo decisões táticas e estratégicas, típico do controle de nível médio.\n    \\end{itemize}\n    \n    \\item **Zhou et al. (2024)** - **Tomada de Decisão de Nível Alto (HL)**\n    \\begin{itemize}\n        \\item Justificativa: O estudo se concentra na interação entre múltiplos agentes em um modelo de aprendizado por reforço profundo, onde cada agente seleciona ações com base em políticas e valores de estado. A ênfase na atualização de pesos de uma rede neural comum e na tomada de decisões em um contexto multiagente indica um nível de decisão mais elevado, característico da tomada de decisão de nível alto.\n    \\end{itemize}\n    \n    \\item **Zheng et al. (2024)** - **Tomada de Decisão de Nível Alto (HL)**\n    \\begin{itemize}\n        \\item Justificativa: O trabalho discute métodos que dependem do conhecimento prévio de especialistas para construir uma base de regras, além de explorar o espaço de busca de políticas através de métodos não baseados em regras. A flexibilidade e a capacidade de resposta imediata proporcionadas pelo aprendizado por reforço para resolver problemas complexos de combate aéreo indicam uma aplicação de inteligência artificial em um nível de decisão elevado.\n    \\end{itemize}\n\\end{itemize}",
      "context": "[Fonte: ID6_Chen et al. - 2024 - Robust Multi-Agent Reinforcement Learning Method Based on Adversarial Domain Randomization for Real-.pdf]\nCalculate the critic lossL of the experiencee;\nCalculate P for each experiencee based on(14);\nGenerate a random numberrd within [0,1];\nif rd <P then\nInsert experiencee into theBatchupdate;\nUpdate theC for E;\nend if\nif Batchupdate is fullthen\nbreak;\nend if\nend for\nend while\nThe software programs and hardware devices can be ho-\nmogeneously and distributively deployed on multiple UA Vs,\nrequiring only simple conﬁgurations to enable any two of them\nto cooperate on the task.\nFurthermore, the software program can be divided into three\nlayers: (1) Data Perception, (2) Action Decision, and (3) Low-\nlevel Control. The last layer is responsible for executing complex\nﬂight actions of UA V and obtaining video streams and sensor\ndata. The 1st layer performs the target recognition on video\nstream data and processes GPS and other information. The 2nd\nlayer makes top-level decisions of a UA V based on the data\noutput by the data perception layer. To apply the intelligent\n\n[Fonte: ID3_Xiao et al. - 2024 - A deep reinforcement learning based distributed multi-UAV dynamic area coverage algorithm for comple.pdf]\nNeurocomputing 595 (2024) 127904\n13\nJ. Xiao et al.\nengineering applications. However, due to the constraints of the input\ndimension of the policy network, the learned policy model is difficult\nto adapt to dynamic scale AOI. To further improve the practicality of\nour algorithm, solving the dynamic scale AOI area coverage is the focus\nof our future work.\nCRediT authorship contribution statement\nJian Xiao:Conceptualization, Data curation, Investigation, Method-\nology, Resources, Validation, Visualization, Writing – original draft,\nWriting – review & editing. Guohui Yuan:Conceptualization, Funding\nacquisition, Investigation, Supervision, Writing – review & editing.\nYuxi Xue:Formal analysis, Investigation, Methodology, Visualization,\nWriting – original draft. Jinhui He:Conceptualization, Data curation,\nInvestigation, Visualization. Yaoting Wang:Data curation, Validation,\nVisualization, Writing – original draft. Yuanjiang Zou: Conceptual-\n\n[Fonte: ID6_Chen et al. - 2024 - Robust Multi-Agent Reinforcement Learning Method Based on Adversarial Domain Randomization for Real-.pdf]\nstream data and processes GPS and other information. The 2nd\nlayer makes top-level decisions of a UA V based on the data\noutput by the data perception layer. To apply the intelligent\npolicy trained in a simulation environment to a real-world control\nAuthorized licensed use limited to: INSTITUTO MILITAR DE ENGENHARIA. Downloaded on November 28,2024 at 01:00:32 UTC from IEEE Xplore.  Restrictions apply.\n\n[Fonte: ID1_Zhou et al. - 2024 - P-DRL A Framework for Multi-UAVs Dynamic Formation Control under Operational Uncertainty and Unknow.pdf]\nFigure 2. Reward shaping with collision avoidance enhanced.\n3.4.4. Interaction\nThis is similar to the architecture of most DRL models; for each timestampt, the Agent\nselects the action At based on the policy and the q/v-value of the state St evaluated by\nthe neural networks, then obtains the reward Rt, and the state of UAV pair becomes\nSt+1. In the multi-agent learning model of A3C, it can be regarded that n Agents are\nperforming the above steps and updating the weights of their common neural network, as\nseen in Figures 3 and 4.\n\n[Fonte: ID2_Zheng et al. - 2024 - Mean policy-based proximal policy optimization for maneuvering decision in multi-UAV air combat.pdf]\nmethods rely on the prior expert knowledge to build a rule\nbase representing the policy, which is then optimized by\nfuzzy tree search [ 12], genetic algorithm [ 13], rough set\nmethod [ 14], and Bayesian inference [ 15]. However, these\nmethods rely on expensive expert knowledge, which\nrestricts the search space of policy and results in a lack of\nexploration. Non-rule-based methods, on the other hand,\nexplore the policy searching space through metaheuristic\nsearch [ 16], dynamic programming [ 17], and model pre-\ndictive control [ 18].\nDue to the remarkable ﬂexibility, and the immediacy of\nend-to-end output by combining with deep learning, rein-\nforcement learning (RL) has received increasing interests\nin resolving the AAMD problem in recent years. For one-\non-one problems, Sun et al. [ 19] proposed a multi-agent\nhierarchical policy gradient algorithm (MAHPG) to learn\nthe multi-level decision-making. Chai et al. [ 20] proposed a\nhierarchical proximal policy optimization (PPO) method\n\n[Fonte: ID1_Zhou et al. - 2024 - P-DRL A Framework for Multi-UAVs Dynamic Formation Control under Operational Uncertainty and Unknow.pdf]\nheuristic control algorithms, as well as artificial intelligence (AI) control algorithms [21,22]\nin recent years. The existing research can be classified into two main categories from the\nperspective of UA V formation control algorithms: cybernetics and artificial intelligence.",
      "model": "gpt-4o-mini",
      "top_k": 6
    },
    {
      "timestamp": "2025-12-23T19:46:12.302617",
      "question": "\n    List for me the papers analysed with name, authors and year.\n",
      "answer": "Os artigos analisados são os seguintes:\n\n1. **ID_13** - Autores: Ruipeng Tang, Jianrui Tang, Mohamad Sofian Abu Talip, Narendra Kumar Aridas, Xifeng Xu - Ano: 2020.\n2. **ID11** - Autores: S. Qamar et al. - Ano: 2022.",
      "context": "[Fonte: ID_13.pdf]\nof Civil Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur 50603, Malaysia. email:  \n22057874@siswa.um.edu.my\nOPEN\nScientific Reports |         (2025) 15:9139 1| https://doi.org/10.1038/s41598-025-88145-7\nwww.nature.com/scientificreports\n\n[Fonte: ID11_S. Qamar et al. - 2022 - Autonomous Drone Swarm Navigation and Multitarget Tracking With Island Policy-Based Optimization Fra.pdf]\nprocessing, the AIoT, fuzzy sets and systems, evolutionary algorithms,\noptimization, and relevant applications of medical and visual surveillance\nsystems.\nASIFULLAH KHAN became a Full Professor,\nin 2016. He has more than 21 years of research\nexperience and is currently working as a Profes-\nsor with the PIEAS. He has been the Head of\nthe Department of Computer and Information Sci-\nences, PIEAS, from 2016 to 2019, where he is\nalso the Head of the PIEAS AI Center. In the\nﬁeld of machine learning and pattern recognition,\nhe has 110 international journals, 53 conferences,\nand nine book chapter publications with more than\n6000 citations to his credit. He has successfully supervised 20 Ph.D. scholars\nso far and is on the panel of reviewers of 53 ISI international journals.\nHe has won eight research grants as a principal investigator. His research\ninterests include machine learning, deep neural networks, image processing,\nand pattern recognition. He has been awarded the President’s Award for\n\n[Fonte: ID_13.pdf]\ndelays under antagonistic interactions. Sci. China Inf. Sci. 63, 1–13 (2020).\nAuthor contributions\nRuipeng Tang: Conceptualization, Methodology, Software, Validation, Formal Analysis, Investigation, Data Cu-\nration, Writing—Original Draft, Writing—Review and Editing, Visualization. Jianrui Tang: Conceptualization, \nMethodology, Validation, Formal Analysis, Writing—Review and Editing. Visualization. Mohamad Sofian Abu \nTalip: Investigation, Writing—Review and Editing, Supervision. Narendra Kumar Aridas: Resources, Project \nAdministration, Methodology. Xifeng Xu: Data Curation, Software, Note: All the above authors agree to be re-\nsponsible for the content and conclusions of the article.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to R.T.\nScientific Reports |         (2025) 15:9139 16| https://doi.org/10.1038/s41598-025-88145-7\nwww.nature.com/scientificreports/\n\n[Fonte: ID11_S. Qamar et al. - 2022 - Autonomous Drone Swarm Navigation and Multitarget Tracking With Island Policy-Based Optimization Fra.pdf]\ninterests include machine learning, deep neural networks, image processing,\nand pattern recognition. He has been awarded the President’s Award for\nPride of Performance in 2018. In addition, he has received four HECś\nOutstanding Research Awards and one Best University Teachers Award.\nHe has also received PAS-COMSTECH Prize 2011 in computer science &\nIT. He has received Research Productivity Awards from the Pakistan Council\nfor Science and Technology (PCST) in 2012, 2013, 2014, 2015, and 2016.\nVOLUME 10, 2022 91091\n\n[Fonte: 03_rev_literatura.tex]\n\\subsubsection{Prevenção de Colisões e Navegação}\nTécnicas como as descritas em \\cite{ID7} e \\cite{ID11} priorizam a navegação segura e a prevenção de obstáculos por meio de mecanismos avançados de recompensa e frameworks de treinamento com RL. A abordagem DRL em duas etapas apresentada em \\cite{ID7}, incorporando experiência de especialistas, melhora a velocidade de convergência e a capacidade de generalização em ambientes dinâmicos. Isso reduz os custos de treinamento, garantindo uma navegação confiável para múltiplos agentes.\n\\subsection{Melhorias de Desempenho e Trade-Offs}\n\\subsubsection{Escalabilidade}\n\n[Fonte: ID6_Chen et al. - 2024 - Robust Multi-Agent Reinforcement Learning Method Based on Adversarial Domain Randomization for Real-.pdf]\nsession Chair, or Steering Committee Chair of many international conferences.\nHe was an Associate Editor for the IEEE TRANSACTIONS ON SYSTEMS,M AN,\nAND CYBERNETICS–PART C:A PPLICATIONS ANDREVIEWS PUBLICATION INFOR-\nMATION. He is an Associate Editor for IEEE/CAA JOURNAL OF AUTOMATICA\nSINICA.\nAuthorized licensed use limited to: INSTITUTO MILITAR DE ENGENHARIA. Downloaded on November 28,2024 at 01:00:32 UTC from IEEE Xplore.  Restrictions apply.",
      "model": "gpt-4o-mini",
      "top_k": 6
    },
    {
      "timestamp": "2025-12-23T19:47:53.268766",
      "question": "\n    How many papers were analysed in the provided context?\n",
      "answer": "No contexto fornecido, foram analisados cinco trabalhos.",
      "context": "[Fonte: 03_rev_literatura.tex]\n\\subsubsection{Prevenção de Colisões e Navegação}\nTécnicas como as descritas em \\cite{ID7} e \\cite{ID11} priorizam a navegação segura e a prevenção de obstáculos por meio de mecanismos avançados de recompensa e frameworks de treinamento com RL. A abordagem DRL em duas etapas apresentada em \\cite{ID7}, incorporando experiência de especialistas, melhora a velocidade de convergência e a capacidade de generalização em ambientes dinâmicos. Isso reduz os custos de treinamento, garantindo uma navegação confiável para múltiplos agentes.\n\\subsection{Melhorias de Desempenho e Trade-Offs}\n\\subsubsection{Escalabilidade}\n\n[Fonte: ID_13.pdf]\ndelays under antagonistic interactions. Sci. China Inf. Sci. 63, 1–13 (2020).\nAuthor contributions\nRuipeng Tang: Conceptualization, Methodology, Software, Validation, Formal Analysis, Investigation, Data Cu-\nration, Writing—Original Draft, Writing—Review and Editing, Visualization. Jianrui Tang: Conceptualization, \nMethodology, Validation, Formal Analysis, Writing—Review and Editing. Visualization. Mohamad Sofian Abu \nTalip: Investigation, Writing—Review and Editing, Supervision. Narendra Kumar Aridas: Resources, Project \nAdministration, Methodology. Xifeng Xu: Data Curation, Software, Note: All the above authors agree to be re-\nsponsible for the content and conclusions of the article.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to R.T.\nScientific Reports |         (2025) 15:9139 16| https://doi.org/10.1038/s41598-025-88145-7\nwww.nature.com/scientificreports/\n\n[Fonte: 03_rev_literatura.tex]\n\\chapter{Trabalhos Relacionados}\nNeste capítulo, são apresentados os trabalhos identificados a partir de uma revisão sistemática da literatura. É importante ressaltar que os trabalhos incluídos aqui representam apenas um extrato da literatura acadêmica, servindo como referencial para o desenvolvimento desta dissertação.\n\\section{Revisão Sistemática da Literatura}\nEsta revisão foca especificamente em abordagens baseadas em IA, particularmente Aprendizado por Reforço (RL) e Aprendizado Profundo (DL), em vez de oferecer uma análise ampla dos métodos tradicionais. Esse foco é motivado pelo crescente consenso na literatura de que as técnicas de IA são essenciais para superar as limitações das abordagens convencionais na adaptação a ambientes complexos e dinâmicos. Os VANTs modernos agora são equipados com processadores poderosos e sensores avançados, permitindo tomada de decisão em tempo real e ajustes dinâmicos às mudanças ambientais. Os objetivos que esta revisão busca enfatizar são:\n\n[Fonte: ID3_Xiao et al. - 2024 - A deep reinforcement learning based distributed multi-UAV dynamic area coverage algorithm for comple.pdf]\nNeurocomputing 595 (2024) 127904\n13\nJ. Xiao et al.\nengineering applications. However, due to the constraints of the input\ndimension of the policy network, the learned policy model is difficult\nto adapt to dynamic scale AOI. To further improve the practicality of\nour algorithm, solving the dynamic scale AOI area coverage is the focus\nof our future work.\nCRediT authorship contribution statement\nJian Xiao:Conceptualization, Data curation, Investigation, Method-\nology, Resources, Validation, Visualization, Writing – original draft,\nWriting – review & editing. Guohui Yuan:Conceptualization, Funding\nacquisition, Investigation, Supervision, Writing – review & editing.\nYuxi Xue:Formal analysis, Investigation, Methodology, Visualization,\nWriting – original draft. Jinhui He:Conceptualization, Data curation,\nInvestigation, Visualization. Yaoting Wang:Data curation, Validation,\nVisualization, Writing – original draft. Yuanjiang Zou: Conceptual-\n\n[Fonte: 03_rev_literatura.tex]\n\\subsubsection{Alocação de Tarefas e Cobertura de Área}\nA alocação de tarefas e a cobertura de área emergem como áreas de foco críticas em \\cite{ID4} e \\cite{ID3}. Os algoritmos baseados em DRL propostos distribuem de forma eficiente as tarefas de sensoriamento e ajustam dinamicamente os pontos de cobertura. Por exemplo, o método de alocação de tarefas Pareto-ótimo em \\cite{ID4} combina a colaboração entre VANTs e humanos, enfatizando aplicações no mundo real, como cidades inteligentes e serviços públicos. Esses estudos ressaltam o potencial das técnicas de DL na otimização do desempenho das tarefas, ao mesmo tempo em que lidam com restrições como comunicação e poder computacional.\n\\subsubsection{Prevenção de Colisões e Navegação}\n\n[Fonte: ID11_S. Qamar et al. - 2022 - Autonomous Drone Swarm Navigation and Multitarget Tracking With Island Policy-Based Optimization Fra.pdf]\nprocessing, the AIoT, fuzzy sets and systems, evolutionary algorithms,\noptimization, and relevant applications of medical and visual surveillance\nsystems.\nASIFULLAH KHAN became a Full Professor,\nin 2016. He has more than 21 years of research\nexperience and is currently working as a Profes-\nsor with the PIEAS. He has been the Head of\nthe Department of Computer and Information Sci-\nences, PIEAS, from 2016 to 2019, where he is\nalso the Head of the PIEAS AI Center. In the\nﬁeld of machine learning and pattern recognition,\nhe has 110 international journals, 53 conferences,\nand nine book chapter publications with more than\n6000 citations to his credit. He has successfully supervised 20 Ph.D. scholars\nso far and is on the panel of reviewers of 53 ISI international journals.\nHe has won eight research grants as a principal investigator. His research\ninterests include machine learning, deep neural networks, image processing,\nand pattern recognition. He has been awarded the President’s Award for",
      "model": "gpt-4o-mini",
      "top_k": 6
    },
    {
      "timestamp": "2025-12-23T19:55:10.931169",
      "question": "\n    Analise a estrutura e a organização do Capítulo 3 (Trabalhos Relacionados)\ncom base exclusivamente no contexto fornecido.\n\nAvalie:\n- o encadeamento lógico entre seções e subseções;\n- se a estrutura atual sustenta de forma clara as questões de pesquisa definidas;\n- possíveis redundâncias ou sobreposições conceituais entre seções;\n- elementos estruturais ausentes ou pouco explorados em uma revisão sistemática;\n- oportunidades de melhorar clareza, coerência e legibilidade para a banca.\n\nNão reescreva o capítulo.\nNão introduza novos trabalhos ou referências.\nForneça apenas recomendações estruturais e sugestões de reorganização.\n\nApresente a resposta como uma lista estruturada de recomendações,\nadequada para uma dissertação de mestrado.\n",
      "answer": "**Recomendações para a Estrutura e Organização do Capítulo 3 (Trabalhos Relacionados)**\n\n1. **Encadeamento Lógico:**\n   - Avaliar a transição entre as seções e subseções para garantir que cada tópico flua naturalmente para o próximo. A introdução de conectores lógicos pode ajudar a guiar o leitor através das ideias apresentadas.\n\n2. **Clareza nas Questões de Pesquisa:**\n   - As seções devem ser revisadas para assegurar que cada uma delas se relacione diretamente com as questões de pesquisa definidas. Considerar a inclusão de um parágrafo introdutório em cada seção que explique como o conteúdo se conecta às questões de pesquisa.\n\n3. **Redundâncias e Sobreposições:**\n   - Identificar e eliminar possíveis redundâncias, especialmente nas subseções que tratam de temas semelhantes, como \"Taxa de Sucesso\" e \"Prevenção de Colisões e Navegação\". Consolidar informações que se sobrepõem pode aumentar a clareza e a concisão.\n\n4. **Elementos Estruturais Ausentes:**\n   - Incluir uma subseção que discuta as limitações dos estudos revisados, o que pode enriquecer a análise crítica e fornecer uma visão mais equilibrada sobre o estado atual da pesquisa.\n\n5. **Exploração de Temas Relevantes:**\n   - Considerar a adição de uma subseção que aborde as implicações práticas das abordagens discutidas, como a aplicação em cenários do mundo real, para conectar a pesquisa acadêmica com aplicações práticas.\n\n6. **Melhoria da Coerência e Legibilidade:**\n   - Revisar a linguagem utilizada para garantir que seja acessível, mantendo um tom acadêmico. O uso de definições claras e exemplos concretos pode ajudar na compreensão dos conceitos complexos.\n\n7. **Organização das Seções:**\n   - Reorganizar as subseções de forma a agrupar temas relacionados, como \"Resiliência\" e \"Tempo de Execução\", que podem ser discutidos em conjunto, uma vez que ambos abordam a eficácia em ambientes dinâmicos.\n\n8. **Sumário das Contribuições:**\n   - Incluir um parágrafo de conclusão ao final do capítulo que sintetize as principais descobertas e contribuições dos trabalhos revisados, reforçando a relevância para o desenvolvimento da dissertação.\n\nEssas recomendações visam aprimorar a estrutura e a clareza do Capítulo 3, facilitando a compreensão e a análise crítica por parte da banca examinadora.",
      "context": "[Fonte: 03_rev_literatura.tex]\n\\chapter{Trabalhos Relacionados}\nNeste capítulo, são apresentados os trabalhos identificados a partir de uma revisão sistemática da literatura. É importante ressaltar que os trabalhos incluídos aqui representam apenas um extrato da literatura acadêmica, servindo como referencial para o desenvolvimento desta dissertação.\n\\section{Revisão Sistemática da Literatura}\nEsta revisão foca especificamente em abordagens baseadas em IA, particularmente Aprendizado por Reforço (RL) e Aprendizado Profundo (DL), em vez de oferecer uma análise ampla dos métodos tradicionais. Esse foco é motivado pelo crescente consenso na literatura de que as técnicas de IA são essenciais para superar as limitações das abordagens convencionais na adaptação a ambientes complexos e dinâmicos. Os VANTs modernos agora são equipados com processadores poderosos e sensores avançados, permitindo tomada de decisão em tempo real e ajustes dinâmicos às mudanças ambientais. Os objetivos que esta revisão busca enfatizar são:\n\n[Fonte: 03_rev_literatura.tex]\n\\subsubsection{Taxa de Sucesso}\nO aprimoramento da taxa de sucesso em tarefas do enxame, como prevenção de colisões e alocação dinâmica de tarefas, foi um foco central. Por exemplo, \\cite{ID2} relatou maior estabilidade e eficiência na tomada de decisão cooperativa utilizando o algoritmo MP3O, melhorando as taxas de sucesso em manobras de combate aéreo. Além disso, \\cite{ID7} demonstrou que o pré-treinamento de agentes com dados especializados levou a uma convergência mais rápida e a resultados de rastreamento mais confiáveis, especialmente em ambientes com obstáculos. No entanto, esses sucessos geralmente dependem de um design cuidadoso das funções de recompensa e otimizações específicas do domínio, o que pode limitar a generalização dos métodos.\n\\subsubsection{Resiliência}\n\n[Fonte: 03_rev_literatura.tex]\n\\subsubsection{Prevenção de Colisões e Navegação}\nTécnicas como as descritas em \\cite{ID7} e \\cite{ID11} priorizam a navegação segura e a prevenção de obstáculos por meio de mecanismos avançados de recompensa e frameworks de treinamento com RL. A abordagem DRL em duas etapas apresentada em \\cite{ID7}, incorporando experiência de especialistas, melhora a velocidade de convergência e a capacidade de generalização em ambientes dinâmicos. Isso reduz os custos de treinamento, garantindo uma navegação confiável para múltiplos agentes.\n\\subsection{Melhorias de Desempenho e Trade-Offs}\n\\subsubsection{Escalabilidade}\n\n[Fonte: 03_rev_literatura.tex]\n\\subsection{Melhorias de Desempenho e Trade-Offs}\n\\subsubsection{Escalabilidade}\nA escalabilidade, um desafio crítico em sistemas de enxame de VANTs, foi abordada de forma eficaz em diversos estudos. Por exemplo, o framework P-DRL em \\cite{ID1} demonstrou controle de formação em tempo real para enxames compostos por 10–20 VANTs, ampliando significativamente a escalabilidade da coordenação multiagente. Da mesma forma, \\cite{ID11} introduziu um conceito de enxame dinâmico que permitiu a partição e reagrupamento de grupos de VANTs de forma adaptativa, garantindo flexibilidade para atender às mudanças nos requisitos das tarefas. Essas abordagens ressaltam o potencial dos métodos de RL na expansão das operações do enxame sem comprometer o desempenho, embora frequentemente exijam recursos computacionais substanciais durante o treinamento.\n\\subsubsection{Tempo de Execução}\n\n[Fonte: 03_rev_literatura.tex]\n\\subsubsection{Tempo de Execução}\nA redução do tempo de execução para tarefas como planejamento dinâmico de trajetória e controle de formação é essencial para aplicações em tempo real. Estudos como \\cite{ID5} utilizaram aprendizado por reforço profundo multitarefa (MT-DDPG) para acelerar a conclusão de operações de pastoreio. Da mesma forma, \\cite{ID9} empregou modelos leves de DL, como o YOLOv7, para realizar controle de formação baseado em visão em tempo real, demonstrando tempos de execução adequados para processamento embarcado. No entanto, essas melhorias aumentam a complexidade algorítmica e exigem treinamento intensivo em recursos computacionais.\n\\subsubsection{Taxa de Sucesso}\n\n[Fonte: 03_rev_literatura.tex]\n\\subsubsection{Resiliência}\nA resiliência a incertezas ambientais e falhas do sistema foi destacada em estudos que empregam técnicas de randomização de domínio. \\cite{ID6} propôs a randomização adversarial de domínio para melhorar a robustez das políticas MARL na transição de simulação para realidade, garantindo desempenho consistente em diversos cenários operacionais. Da mesma forma, \\cite{ID8} introduziu um framework de perseguição e evasão que manteve o sucesso da missão mesmo quando alguns VANTs do enxame foram comprometidos. Embora esses métodos aumentem a resiliência, geralmente exigem alto custo computacional e ambientes de simulação extensivos para um treinamento eficaz.\n\\subsubsection{Trade-Offs}\n\\begin{itemize}\n\n[Fonte: 03_rev_literatura.tex]\n\\subsubsection{Alocação de Tarefas e Cobertura de Área}\nA alocação de tarefas e a cobertura de área emergem como áreas de foco críticas em \\cite{ID4} e \\cite{ID3}. Os algoritmos baseados em DRL propostos distribuem de forma eficiente as tarefas de sensoriamento e ajustam dinamicamente os pontos de cobertura. Por exemplo, o método de alocação de tarefas Pareto-ótimo em \\cite{ID4} combina a colaboração entre VANTs e humanos, enfatizando aplicações no mundo real, como cidades inteligentes e serviços públicos. Esses estudos ressaltam o potencial das técnicas de DL na otimização do desempenho das tarefas, ao mesmo tempo em que lidam com restrições como comunicação e poder computacional.\n\\subsubsection{Prevenção de Colisões e Navegação}\n\n[Fonte: 03_rev_literatura.tex]\n\\item \\textbf{RQ5}: Como as abordagens de RL e DL melhoram a eficiência energética, a taxa de sucesso das missões e a adaptabilidade em sistemas de enxame de VANTs?\n    \\item \\textbf{RQ6}: Até que ponto os métodos de RL e DL são validados em cenários reais de enxames de VANTs e quais são os desafios para reduzir a lacuna entre simulação e implementação?\n\\end{enumerate}\n\\section{Metodologia}\n O processo de busca para esta revisão sistemática foi orientado pelo framework PICO (População, Intervenção, Comparação, Resultado), uma metodologia amplamente estabelecida e utilizada em revisões sistemáticas para desenvolver estratégias de busca abrangentes e focadas. O framework PICO facilitou a criação de uma string de busca projetada para capturar os estudos mais relevantes na área de sistemas de enxame de VANTs baseados em IA.\n \\begin{table}[ht]\n\\centering\n\\caption{Construção da string de busca utilizando o framework PICO}\n\\begin{tabularx}{\\columnwidth}{|c|c|X|}\n\\hline\n\n[Fonte: 03_rev_literatura.tex]\n\\item \\textbf{Exclusões Baseadas na Metodologia}: Estudos com metodologias vagas ou incompletas, como ausência de detalhes sobre algoritmos, simulações ou configurações experimentais, foram excluídos para garantir a inclusão de pesquisas reproduzíveis e confiáveis. Artigos não revisados por pares foram excluídos, a menos que fossem pré-prints relevantes de repositórios renomados, como o arXiv, garantindo o rigor científico dos estudos incluídos.\n    \\item \\textbf{Exclusões Baseadas na Aplicação}: Artigos focados em aplicações ou tarefas fora do escopo do comportamento de enxames de VANTs (por exemplo, aplicações industriais, formações de robôs terrestres) foram excluídos. Estudos que não demonstram resultados de simulação em um ambiente 3D ou que não apresentam validação experimental foram excluídos, pois esses aspectos são essenciais para avaliar a aplicabilidade no mundo real e a escalabilidade das abordagens analisadas.\n\\end{itemize}\n\n[Fonte: 03_rev_literatura.tex]\n\\end{itemize}\n\\section{Discussão}\n\\subsection{Principais Descobertas}\nEsta revisão sistemática explorou as contribuições das técnicas de Aprendizado por Reforço (RL) e Aprendizado Profundo (DL) para o avanço dos sistemas de enxame de VANTs, destacando seu potencial transformador na solução de desafios críticos. Nos artigos revisados, foram alcançadas melhorias significativas em escalabilidade, tempo de execução, taxa de sucesso e resiliência. As principais contribuições incluem:\n\\begin{itemize}\n    \\item \\textbf{Controle Dinâmico de Formação}: Estudos como \\cite{ID1} demonstraram frameworks capazes de realizar controle de formação em tempo real para enxames de VANTs de médio porte, reduzindo a complexidade computacional por meio de algoritmos avançados de RL, como A3C e DQN.",
      "model": "gpt-4o-mini",
      "top_k": 10
    }
  ]
}